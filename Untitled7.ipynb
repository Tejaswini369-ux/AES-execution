{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejaswini369-ux/AES-execution/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR_LcyPCPV0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59e6b0a-8c72-4198-e14d-9736f93e05cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['audios', 'normalized_audios', 'videos', 'spectrograms', 'models', 'embeddings', 'texture_images', 'landmarks', 'source_landmarks']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create folders\n",
        "folders = ['audios','normalized_audios', 'videos', 'spectrograms', 'models','embeddings','texture_images','landmarks','source_landmarks']\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Check if folders were created\n",
        "created_folders = [folder for folder in folders if os.path.exists(folder)]\n",
        "print(created_folders)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def extract_wav(id, verbose=False, duration=6, sample_rate=16000):\n",
        "    \"\"\"\n",
        "    Extract audio from a video file and save it as a WAV file.\n",
        "\n",
        "    Parameters:\n",
        "    - id: The ID of the video file without the extension.\n",
        "    - verbose: Whether to print verbose messages.\n",
        "    - duration: Duration of the audio to extract in seconds.\n",
        "    - sample_rate: Sample rate for the audio.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"-----------extracting audio-------------\")\n",
        "\n",
        "    wavfile = id + \".wav\"\n",
        "    videos_path = \"videos/\"\n",
        "    audios_path = \"audios/\"\n",
        "\n",
        "    if not os.path.isfile(audios_path + wavfile):\n",
        "        # Extract audio using ffmpeg directly to audios directory\n",
        "        os.popen(f\"ffmpeg -nostats -loglevel 0 -t {duration} -stream_loop -1 -i {videos_path}{id}.mp4 -vn -ac 1 -ar {sample_rate} -acodec pcm_s16le {audios_path}{wavfile}\").read()\n",
        "\n",
        "        if not os.path.isfile(audios_path + wavfile):\n",
        "            if verbose:\n",
        "                print(\"----------------ffmpeg can't extract audio so deleting --------------\")\n",
        "            return 1\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(f\"skipping audio extraction for {id}\")"
      ],
      "metadata": {
        "id": "NS295NGCPgI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python ffmpeg-normalize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOMHTc2fwuhs",
        "outputId": "ded3414c-e9c7-4757-98d6-26aa377abf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting ffmpeg-normalize\n",
            "  Using cached ffmpeg_normalize-1.32.2-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ffmpeg-normalize) (4.67.1)\n",
            "Requirement already satisfied: ffmpeg-progress-yield in /usr/local/lib/python3.11/dist-packages (from ffmpeg-normalize) (0.12.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ffmpeg-normalize) (6.9.0)\n",
            "Requirement already satisfied: mutagen in /usr/local/lib/python3.11/dist-packages (from ffmpeg-normalize) (1.47.0)\n",
            "Using cached ffmpeg_normalize-1.32.2-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: ffmpeg-normalize\n",
            "Successfully installed ffmpeg-normalize-1.32.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ffmpeg\n",
        "import subprocess\n",
        "\n",
        "def normalize_audio(id):\n",
        "    input_path = os.path.join('audios', f'{id}.wav')\n",
        "    output_dir = 'normalized_audios'\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
        "    output_path = os.path.join(output_dir, f'{id}.wav')\n",
        "\n",
        "    # Convert to 16kHz, mono, 16-bit PCM WAV\n",
        "    ffmpeg.input(input_path).output(\n",
        "        output_path,\n",
        "        ar=16000,\n",
        "        acodec='pcm_s16le',\n",
        "        ac=1,\n",
        "        loglevel='quiet'\n",
        "    ).run(overwrite_output=True)\n",
        "\n",
        "    # Normalize audio using ffmpeg-normalize\n",
        "    normalize_command = [\n",
        "        'ffmpeg-normalize', output_path,\n",
        "        '--normalization-type', 'rms',\n",
        "        '-t', '-23',\n",
        "        '-o', output_path,\n",
        "        '-f'\n",
        "    ]\n",
        "    subprocess.run(normalize_command, check=True)\n",
        "\n",
        "    return output_path\n"
      ],
      "metadata": {
        "id": "zTkEImyWw1B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def stretch_audio(waveform, sampling_rate=16000, audio_length=10.26):\n",
        "    target_samples = int(audio_length * sampling_rate)\n",
        "    current_samples = len(waveform)\n",
        "\n",
        "    # Pad with repeating initial segment if too short\n",
        "    if current_samples < target_samples:\n",
        "        padding_needed = target_samples - current_samples\n",
        "        padding = waveform[:padding_needed]  # Get initial portion for repeating\n",
        "\n",
        "        while padding_needed > 0:\n",
        "            add_samples = min(len(padding), padding_needed)\n",
        "            waveform = np.concatenate((waveform, padding[:add_samples]))\n",
        "            padding_needed -= add_samples\n",
        "\n",
        "    # Trim to exact length if too long\n",
        "    return waveform[:target_samples]\n"
      ],
      "metadata": {
        "id": "-wSx5drd1QsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from transformers import AutoModelForAudioClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load base model\n",
        "ast = AutoModelForAudioClassification.from_pretrained(\n",
        "    \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
        "    num_labels=4096,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Modify classifier\n",
        "head = ast.classifier\n",
        "new_head = nn.Sequential(head, nn.ReLU())\n",
        "ast.classifier = new_head\n",
        "ast_checkpoint = torch.load('models/ast.pt', map_location=device)\n",
        "ast.load_state_dict(ast_checkpoint[\"model_state_dict\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4tr2MRXc0RT",
        "outputId": "7038ef47-a09c-4b5c-eeea-13b29d5f89b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
            "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
            "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ast.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VjMEEoqnVkn",
        "outputId": "511a1877-0157-43a1-b0ef-317d74f819f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ASTForAudioClassification(\n",
              "  (audio_spectrogram_transformer): ASTModel(\n",
              "    (embeddings): ASTEmbeddings(\n",
              "      (patch_embeddings): ASTPatchEmbeddings(\n",
              "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ASTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ASTLayer(\n",
              "          (attention): ASTAttention(\n",
              "            (attention): ASTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): ASTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ASTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ASTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): ASTMLPHead(\n",
              "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dense): Linear(in_features=768, out_features=4096, bias=True)\n",
              "    )\n",
              "    (1): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class FaceDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Input to the model should be face embedding vector of dimension equal to 4096\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(FaceDecoder, self).__init__()\n",
        "        self.pre_layers = self.define_pre_layers()\n",
        "        self.landmark_layers = self.define_landmark_layers()\n",
        "        self.texture_layers = self.define_texture_layers()\n",
        "\n",
        "\n",
        "    def define_pre_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        pre_layer1 = nn.Sequential(\n",
        "            nn.Linear(in_features=4096, out_features=3072),\n",
        "            nn.BatchNorm1d(num_features=3072),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(pre_layer1)\n",
        "\n",
        "        pre_layer2 = nn.Sequential(\n",
        "            nn.Linear(in_features=3072, out_features=2048),\n",
        "            nn.BatchNorm1d(num_features=2048),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(pre_layer2)\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def define_landmark_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        landmark_layer1 = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=1024),\n",
        "            nn.BatchNorm1d(num_features=1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(landmark_layer1)\n",
        "\n",
        "        landmark_layer2 = nn.Sequential(\n",
        "            nn.Linear(in_features=1024, out_features=512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(landmark_layer2)\n",
        "\n",
        "        landmark_layer3 = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(landmark_layer3)\n",
        "\n",
        "        landmark_layer4 = nn.Sequential(\n",
        "            nn.Linear(in_features=256, out_features=144),\n",
        "        )\n",
        "        layers.append(landmark_layer4)\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def define_texture_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        texture_layer1 = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=256 * 14 * 14),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(texture_layer1)\n",
        "\n",
        "        texture_layer2 = nn.Sequential(\n",
        "            nn.Unflatten(dim=1, unflattened_size=(256, 14, 14)),\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(texture_layer2)\n",
        "\n",
        "        texture_layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(texture_layer3)\n",
        "\n",
        "        texture_layer4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(texture_layer4)\n",
        "\n",
        "        texture_layer5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        layers.append(texture_layer5)\n",
        "\n",
        "        texture_layer6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1, stride=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        layers.append(texture_layer6)\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for pre_layer in self.pre_layers:\n",
        "            x = pre_layer(x)\n",
        "\n",
        "        landmarks = x\n",
        "        for landmark_layer in self.landmark_layers:\n",
        "            landmarks = landmark_layer(landmarks)\n",
        "\n",
        "        texture = x\n",
        "        for texture_layer in self.texture_layers:\n",
        "            texture = texture_layer(texture)\n",
        "\n",
        "        return landmarks, texture\n",
        "\n",
        "    def get_predifined_layer_activation(self, x):\n",
        "        for pre_layer in self.pre_layers:\n",
        "            x = pre_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wwUoNTXA9k5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_decoder = FaceDecoder().to(device)\n",
        "face_decoder_checkpoint = torch.load('models/face_decoder.pt', map_location=device)\n",
        "face_decoder.load_state_dict(face_decoder_checkpoint[\"model_state_dict\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD6DUj58Ap1K",
        "outputId": "668d00a9-f232-47b4-a18a-33f212942ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ASTFeatureExtractor\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "feature_extractor = ASTFeatureExtractor(sampling_rate=16000,mean=-5.460994,std=3.1129124)\n",
        "id = \"5ablueV_1tw\"\n",
        "normalize_audio(id)\n",
        "waveform, _ = librosa.load(f\"normalized_audios/{id}.wav\",duration=10.26, sr=16000, mono=True)\n",
        "waveform = stretch_audio(waveform)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "inputs = feature_extractor(waveform, sampling_rate = 16000, padding = \"max_length\", return_tensors = \"np\")\n",
        "input_values = torch.tensor(inputs.input_values).to(device)"
      ],
      "metadata": {
        "id": "BR-5WcqP14p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_decoder.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfrFOg10EYKS",
        "outputId": "f54b29da-c16f-427b-8526-ac808a18681a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FaceDecoder(\n",
              "  (pre_layers): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=3072, bias=True)\n",
              "      (1): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Linear(in_features=3072, out_features=2048, bias=True)\n",
              "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (landmark_layers): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=144, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (texture_layers): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Linear(in_features=2048, out_features=50176, bias=True)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Unflatten(dim=1, unflattened_size=(256, 14, 14))\n",
              "      (1): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): ConvTranspose2d(64, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "with torch.no_grad():\n",
        "  voice_encoder_embeddings = ast(input_values)\n",
        "  voice_encoder_embeddings = voice_encoder_embeddings.logits\n",
        "  print(voice_encoder_embeddings.shape)\n",
        "  landmarks_predicted, images_predicted = face_decoder(voice_encoder_embeddings)\n",
        "  print(images_predicted)\n",
        "  texture_img = images_predicted.cpu().squeeze().permute(1, 2, 0).numpy()  # Convert to HWC format\n",
        "  texture_img = (texture_img * 255).astype(np.uint8)  # Scale to 0-255 range\n",
        "  Image.fromarray(texture_img).save(f'texture_images/{id}.jpg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eCzwUyQ28zN",
        "outputId": "ec43c650-8134-487f-a2a8-9503088063f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4096])\n",
            "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          ...,\n",
            "          [0.2209, 0.2038, 0.1955,  ..., 0.8359, 0.9365, 0.9679],\n",
            "          [0.2444, 0.2074, 0.1990,  ..., 0.7337, 0.7589, 0.9165],\n",
            "          [0.2578, 0.2225, 0.2169,  ..., 0.6665, 0.6689, 0.7414]],\n",
            "\n",
            "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          ...,\n",
            "          [0.2247, 0.2078, 0.2091,  ..., 0.8245, 0.9210, 0.9612],\n",
            "          [0.2389, 0.2164, 0.2161,  ..., 0.7231, 0.7386, 0.8802],\n",
            "          [0.2391, 0.2186, 0.2245,  ..., 0.6287, 0.6182, 0.6470]],\n",
            "\n",
            "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          ...,\n",
            "          [0.2537, 0.2306, 0.2441,  ..., 0.8177, 0.9216, 0.9669],\n",
            "          [0.2711, 0.2505, 0.2593,  ..., 0.7200, 0.7381, 0.8826],\n",
            "          [0.2555, 0.2359, 0.2598,  ..., 0.6101, 0.5991, 0.6103]]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}